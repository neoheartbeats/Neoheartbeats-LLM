{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "from vllm import LLM, LLMEngine, EngineArgs, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/neoheartbeats/Neoheartbeats/models/llama-3.1-8b-inst\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# m = LLM(model=model_id, download)\n",
    "\n",
    "engine_args = EngineArgs(\n",
    "    model=model_path,\n",
    "    download_dir=model_path,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=32768,\n",
    "    guided_decoding_backend=\"lm-format-enforcer\",\n",
    "    enable_prefix_caching=True,\n",
    "    use_v2_block_manager=True,\n",
    "    seed=369,\n",
    "    enable_chunked_prefill=True,\n",
    "    served_model_name=\"llama-3.1-8b-inst\",\n",
    "    # speculative_model=\"[ngram]\",\n",
    "    # num_speculative_tokens=10,\n",
    "    # ngram_prompt_lookup_max=4,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    presence_penalty=0.25,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 12:53:02 config.py:806] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 08-01 12:53:02 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/home/neoheartbeats/Neoheartbeats/models/llama-3.1-8b-inst', speculative_config=None, tokenizer='/home/neoheartbeats/Neoheartbeats/models/llama-3.1-8b-inst', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/home/neoheartbeats/Neoheartbeats/models/llama-3.1-8b-inst', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='lm-format-enforcer'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=369, served_model_name=llama-3.1-8b-inst, use_v2_block_manager=True, enable_prefix_caching=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 12:53:03 model_runner.py:680] Starting to load model /home/neoheartbeats/Neoheartbeats/models/llama-3.1-8b-inst...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aacad1304d4383af316b74f39dc68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 12:53:09 model_runner.py:692] Loading model weights took 14.9888 GB\n",
      "INFO 08-01 12:53:09 gpu_executor.py:102] # GPU blocks: 12269, # CPU blocks: 2048\n",
      "INFO 08-01 12:53:11 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-01 12:53:11 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-01 12:53:20 model_runner.py:1181] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "engine = LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Who is the president of the United States?\"\n",
    "request_id = 0\n",
    "engine.add_request(\n",
    "   str(request_id),\n",
    "   example_prompt,\n",
    "   params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 13:13:54 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Who is the president of the United States?', prompt_token_ids=[128000, 15546, 374, 279, 4872, 315, 279, 3723, 4273, 30], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joe', token_ids=(13142,), cumulative_logprob=-0.3751278519630432, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1722543206.6707091, last_token_time=1722543234.2298498, first_scheduled_time=1722543233.9090934, first_token_time=1722543234.1933842, time_in_queue=27.238384246826172, finished_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMEngine' object has no attribute 'root_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapi_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cond/envs/neoheartbeats/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py:217\u001b[0m, in \u001b[0;36mrun_server\u001b[0;34m(args, llm_engine)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_server\u001b[39m(args, llm_engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 217\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM API server version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, VLLM_VERSION)\n\u001b[1;32m    220\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, args)\n",
      "File \u001b[0;32m~/cond/envs/neoheartbeats/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py:170\u001b[0m, in \u001b[0;36mbuild_app\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    168\u001b[0m app \u001b[38;5;241m=\u001b[39m fastapi\u001b[38;5;241m.\u001b[39mFastAPI(lifespan\u001b[38;5;241m=\u001b[39mlifespan)\n\u001b[1;32m    169\u001b[0m app\u001b[38;5;241m.\u001b[39minclude_router(router)\n\u001b[0;32m--> 170\u001b[0m app\u001b[38;5;241m.\u001b[39mroot_path \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_path\u001b[49m\n\u001b[1;32m    172\u001b[0m mount_metrics(app)\n\u001b[1;32m    174\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(\n\u001b[1;32m    175\u001b[0m     CORSMiddleware,\n\u001b[1;32m    176\u001b[0m     allow_origins\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mallowed_origins,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     allow_headers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mallowed_headers,\n\u001b[1;32m    180\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLMEngine' object has no attribute 'root_path'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neoheartbeats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
